{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f3b5a4-63e2-4b5b-a0e1-9e7b7ceff4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression \n",
    "# When working with PySpark, also need to start the PySpark session \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# This a local computer custer so always have one cluster\n",
    "spark = SparkSession.builder.appName('practiceML').getOrCreate()\n",
    "# Imput dataset\n",
    "\n",
    "df = spark.read.csv('converted_sf_housing_data.csv', header = True, inferSchema = True)\n",
    "df.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eab425e-66cc-4676-8f4e-f003dfbce539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5780 rows in the training set, and 1366 in the test set\n"
     ]
    }
   ],
   "source": [
    "# Spliting data 80:20 and setting a random seed for  reproducibility, such that if we rerun this code we will \n",
    "# get the same data points going to our train and test datasets, respectively.The value of the seed itself shouldn’t matter,\n",
    "# but data scientists often like setting it to 42 as that is the answer to the Ultimate Question of Life:\n",
    "trainDF, testDF = df.randomSplit([.8, .2], seed=42)\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\"\"\")\n",
    "\n",
    "# if the input data changes, then the result of the split (by random Split()) won’t be the same.\n",
    "# So, you should cache the training data set because you will be accessing it many times \n",
    "# throughout the machine learning process\n",
    "\n",
    "# Linear regression (like many other algorithms in Spark) requires that all the input features are \n",
    "# contained within a single vector in DataFrame. Thus, we need to transform the data.\n",
    "\n",
    "# For the task of putting all of our features into a single vector, we will use the VectorAssembler transformer. \n",
    "# VectorAssembler takes a list of input columns and creates a new DataFrame with an additional column, \n",
    "# which we call features. It combines the values of those input columns into a single vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c21aa-85d4-47bc-8642-448fca09050a",
   "metadata": {},
   "source": [
    "## Single Variate LinearRegression (only predicting using bedroom as input feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea99bbb-0d78-4f62-8fcd-d767c835325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e134ec-981d-4f61-a333-72b3d7b36d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression can be extended to handle multiple independent variables. \n",
    "# If we had three features as input, x = [x1, x2, x3], then we could model y as y ≈ w0 + w1x1+ w2x2+ w3x3 + ε. \n",
    "# In this case, there is a separate coefficient (or weight) for each feature and a single intercept (w0 instead of b here). \n",
    "# The process of estimating the coefficients and intercept for model is called learning (or fitting) \n",
    "# the parameters for the model. \n",
    "\n",
    "# In Spark, LinearRegression is\n",
    "# a type of estimator—it takes in a DataFrame and returns a Model. Estimators learn\n",
    "# parameters from your data, have an estimator_name.fit() method, and are eagerly\n",
    "# evaluated (i.e., kick off Spark jobs), whereas transformers are lazily evaluated. Some\n",
    "# other examples of estimators include Imputer, DecisionTreeClassifier, and Random\n",
    "# ForestRegressor.\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel = lr.fit(vecTrainDF)\n",
    "\n",
    "# lr.fit() returns a LinearRegressionModel (lrModel), which is a transformer. In\n",
    "# other words, the output of an estimator’s fit() method is a transformer. Once the\n",
    "# estimator has learned the parameters, the transformer can apply these parameters to\n",
    "# new data points to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b2b8db-5705-4851-9e7d-3d02f45579a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is price = 123.68*bedrooms + 47.51\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the parameters \n",
    "# In Python\n",
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "print(f\"\"\"The formula for the linear regression line is price = {m}*bedrooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb82a40-5783-4a18-8d18-e73c1e3d9ef9",
   "metadata": {},
   "source": [
    "## Creating a Pipeline\n",
    "If we want to apply model to the test set, then we need to prepare that data in the\n",
    "same way as the training set (i.e., pass it through the vector assembler). Oftentimes\n",
    "data preparation pipelines will have multiple steps, and it becomes cumbersome to\n",
    "remember not only which steps to apply, but also the ordering of the steps. This is the\n",
    "motivation for the Pipeline API: you simply specify the stages you want your data to\n",
    "pass through, in order, and Spark takes care of the processing for you. They provide\n",
    "the user with better code reusability and organization. In Spark, Pipelines are esti‐\n",
    "mators, whereas ```PipelineModels```—fitted ```Pipelines```—are transformers.\n",
    "\n",
    "Another advantage of using the Pipeline API is that it determines which stages are\n",
    "estimators/transformers for you, so you don’t have to worry about specifying\n",
    "```name.fit()``` versus ```name.transform()``` for each of the stages.\n",
    "Since ```pipelineModel``` is a transformer, it is straightforward to apply it to our test data\n",
    "set too:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5a839c-02c2-47b7-933d-bfdadd0ade02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+------------------+\n",
      "|bedrooms|features|price|        prediction|\n",
      "+--------+--------+-----+------------------+\n",
      "|     1.0|   [1.0]| 85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|159.0|171.18598011578285|\n",
      "+--------+--------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the pipeline \n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "# Apply the pipelines \n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ffcf3-4a0c-4c12-ae5a-e5400fca4a8f",
   "metadata": {},
   "source": [
    "## Multi-Variate LinearRegression (Using all features)\n",
    "### One-hot encoding\n",
    "\n",
    "Most machine learning models in MLlib expect numerical values as input, repre‐\n",
    "sented as vectors. To convert categorical values into numeric values, we can use a\n",
    "technique called one-hot encoding (OHE). Suppose we have a column called Animal\n",
    "and we have three types of animals: Dog, Cat, and Fish. We can’t pass the string types\n",
    "into our ML model directly, so we need to assign a numeric mapping, such as this:\n",
    "```\n",
    "Animal = {\"Dog\", \"Cat\", \"Fish\"}\n",
    "\"Dog\" = 1, \"Cat\" = 2, \"Fish\" = 3\n",
    "```\n",
    "However, using this approach we’ve introduced some spurious relationships into our\n",
    "data set that weren’t there before. For example, why did we assign Cat twice the value\n",
    "of Dog? The numeric values we use should not introduce any relationships into our\n",
    "data set. Instead, we want to create a separate column for every distinct value in our\n",
    "Animal column:\n",
    "```\n",
    "\"Dog\" = [ 1, 0, 0]\n",
    "\"Cat\" = [ 0, 1, 0]\n",
    "\"Fish\" = [0, 0, 1]\n",
    "```\n",
    "If the animal is a dog, it has a one in the first column and zeros elsewhere. If it is a cat,\n",
    "it has a one in the second column and zeros elsewhere. The ordering of the columns\n",
    "is irrelevant. If you’ve used pandas before, you’ll note that this does the same thing as\n",
    "```pandas.get_dummies()```.\n",
    "If we had a zoo of 300 animals, would OHE massively increase consumption of mem‐\n",
    "ory/compute resources? Not with Spark! Spark internally uses a ```SparseVector``` when\n",
    "the majority of the entries are 0, as is often the case after OHE, so it does not waste\n",
    "space storing 0 values. Let’s take a look at an example to better understand how\n",
    "SparseVectors work:\n",
    "```\n",
    "DenseVector(0, 0, 0, 7, 0, 2, 0, 0, 0, 0)\n",
    "SparseVector(10, [3, 5], [7, 2])\n",
    "```\n",
    "The ```DenseVector``` in this example contains 10 values, all but 2 of which are 0. To create a ```SparseVector```, we need to keep track of the size of the vector, the indices of the\n",
    "nonzero elements, and the corresponding values at those indices. In this example the\n",
    "size of the vector is 10, there are two nonzero values at indices 3 and 5, and the corresponding \n",
    "values at those indices are 7 and 2.\n",
    "\n",
    "There are a few ways to one-hot encode your data with Spark. A common approach is\n",
    "to use the ```StringIndexer``` and ```OneHotEncoder```. With this approach, the first step is to\n",
    "apply the ```StringIndexer``` estimator to convert categorical values into category indices. \n",
    "These category indices are ordered by label frequencies, so the most frequent\n",
    "label gets index 0, which provides us with reproducible results across various runs of\n",
    "the same data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92a13981-1206-490e-8de5-96e909760e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "# How does the StringIndexer handle new categories\n",
    "# that appear in the test data set, but not in the training data set?\n",
    "\n",
    "# There is a\n",
    "# handleInvalid parameter that specifies how you want to handle them. The options\n",
    "# are skip (filter out rows with invalid data), error (throw an error), or keep (put inva‐\n",
    "# lid data in a special additional bucket, at index numLabels). For this example, we just\n",
    "# skipped the invalid records.\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, outputCols=oheOutputCols)\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e37724e-bad4-4ce3-9aef-e13fe5367ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature preparation and model building into the pipeline\n",
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "972d737a-e2c6-412f-bdf5-9f4c0cdb1932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,22,43,...| 85.0| 55.24365707389188|\n",
      "|(98,[0,3,6,22,43,...| 45.0|23.357685914717877|\n",
      "|(98,[0,3,6,22,43,...| 70.0|28.474464479034395|\n",
      "|(98,[0,3,6,12,42,...|128.0| -91.6079079594947|\n",
      "|(98,[0,3,6,12,43,...|159.0| 95.05688229945372|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Inspecting the parameters \n",
    "\n",
    "# m = round(lrModel.coefficients[0], 2)\n",
    "# b = round(lrModel.intercept, 2)\n",
    "# print(f\"\"\"The formula for the linear regression line is price = {m}*bedrooms + {b}\"\"\")\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdea2b8-add0-4fdf-aad1-7349b05a633c",
   "metadata": {},
   "source": [
    "## Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f97026-5197-4e3e-b641-f08801e0726c",
   "metadata": {},
   "source": [
    "## Root Mean-Square Error (RMSE) Approach\n",
    "RMSE score ranges from 0 to infinity. The closer to the zero, the better. \n",
    "\n",
    "**RMSE mathematical concept**\n",
    "- Compuete the error (difference) ```Error = (Y- ȳ)``` and squared it so that positive and negative residuals do not cancle out  ```Squared Error (SE) = (Y- ȳ)^2```\n",
    "- Sum up the squared error for all ```n``` no of records  ```Sum of Squared Errors (SSE) = Sum(i= 1 to n) (Y- ȳ)^2```\n",
    "- However, the SSE grows with the number of records ```n``` in the data set, so we want to normalize it by the number of records. so, it becomes: ```Mean Squared Error (MSE) = 1/n(Sum(i= 1 to n) (Y- ȳ)^2)```\n",
    "- MSE is on the scale of unit-squared. It requires to take the square root of the MSE to get the error back on the scale of the original unit, which gives the root-mean-square error (RMSE) ```Root Mean Squared Error (RMSE) = Sq.Root of MSE```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b16cd056-4f9b-477c-ad12-a1cce8f88334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.6\n"
     ]
    }
   ],
   "source": [
    "# Let’s evaluate our model using RMSE:\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"price\", \n",
    "    metricName=\"rmse\")\n",
    "\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40572627-6992-4dfd-9fec-b1b33435a8f0",
   "metadata": {},
   "source": [
    "**Interpreting the value of RMSE.***\n",
    "*How do we know if 220.6 is a good value for the RMSE?*\n",
    "The most common approach is to build a simple baseline model and compute its RMSE to compare against. \n",
    "A common baseline model for regression tasks is to compute the average value of the label on the training\n",
    "set ȳ , then predict ȳ for every record in the test data set and compute the resulting RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b49881-da31-4348-aedf-361a7bd49aea",
   "metadata": {},
   "source": [
    "## DBC Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e40913-5e17-4149-9983-470849c71136",
   "metadata": {},
   "source": [
    "The baseline model has RMSE of 240.7. so, we beat baseline. If it doesn't beat the baseline, then something probably\n",
    "went wrong in your model builing process. If this were a classification problem, you might want to predict the\n",
    "most prevalent class as your baseline model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95beed07-2de9-4ad9-a72a-bac99758ae4f",
   "metadata": {},
   "source": [
    "## R-Squared Approach\n",
    "Despite the name R^2 containing “squared,” R-squared values range from negative infinity to 1.\n",
    "\n",
    "# Computation Image from excel \n",
    "\n",
    "If the model perfectly predicts every data point, then the ```RSS = 0```, making ```R-Squared = 1```. And if \n",
    "```RSS = TSS``` then the fraction becomes 1/1 so R-Squared is 0. This is what happens if model performs the same as \n",
    "always predicting the average value YBar. \n",
    "\n",
    "But what if your model performs worse than always predicting ȳ and your RSS isreally large? Then your R-Squared\n",
    "can actually be negative! If your R-Squared is negative, you should reevaluate your modeling process. The nice thing about using R-Squared is that you don’t necessarily need to define a baseline model to compare against.\n",
    "\n",
    "If we want to change our regression evaluator to use R-Squared, instead of redefining the regression evaluator, we can set the metric name using the setter property. \n",
    "\n",
    "```\n",
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b624666e-68e1-4feb-aaa2-f4adca68082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.16043316698848087\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92debad-0eef-497f-9618-b19058e3940e",
   "metadata": {},
   "source": [
    "So, In this example R-Squared is positive, but it is very close to 0. One of the reasons why our model is not performing too well is because our label, price, appears to be ```log-normally distributed``` (https://en.wikipedia.org/wiki/Log-normal_distribution). If a distribution is log-normal, it means that if we take the logarithm of the value, the result looks like a normal distribution. Price is often log-normally distributed. If you think about rental prices in San Francisco, most cost around $200 per night, but there are some that rent for thousands of dollars a night! You can see the distribution of our Airbnb prices for our training Dataset. \n",
    "## Insert fig 10-7 and 10-8 from book page 329 and as well as the code from repo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe784f5-21e0-4f6c-8cb2-65c03ba00ccc",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "A hyperparameter is an attribute that you define about the model prior to training, and it is not learned during the training process (not to be confused with parameters, which are learned in the training process). The number of trees in your random forest is an example of a hyperparameter.\n",
    "Let's focus on using tree-based models as an example for hyperparameter turning procedures, but the same concepts apply to other models as well. \n",
    "\n",
    "**Tree-Based Models**\n",
    "A decision tree is a series of if-then-else rules learned from your data for classification or regression tasks. For decision trees, you don’t have to worry about standardizing or scaling your input features, because this has no impact on the splits—but you do have to be careful about how you prepare your categorical features.Tree-based methods can naturally handle categorical variables. In spark.ml, you just need to pass the categorical columns to the StringIndexer, and the decision tree can take care of the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdac1dbd-a919-4138-81fc-93e1b7273dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_e17af87064a5, depth=5, numNodes=47, numFeatures=33\n",
      "  If (feature 12 <= 2.5)\n",
      "   If (feature 12 <= 1.5)\n",
      "    If (feature 5 in {1.0,2.0})\n",
      "     If (feature 4 in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 104.23992784125075\n",
      "      Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 250.7111111111111\n",
      "     Else (feature 4 not in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 151.94179894179894\n",
      "      Else (feature 3 not in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 245.8507462686567\n",
      "    Else (feature 5 not in {1.0,2.0})\n",
      "     If (feature 3 in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 3 in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 131.96658097686375\n",
      "      Else (feature 3 not in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 164.19959266802445\n",
      "     Else (feature 3 not in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 10 <= 6.5)\n",
      "       Predict: 205.5814889336016\n",
      "      Else (feature 10 > 6.5)\n",
      "       Predict: 841.6666666666666\n",
      "   Else (feature 12 > 1.5)\n",
      "    If (feature 13 <= 4.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 26.5)\n",
      "       Predict: 290.8357933579336\n",
      "      Else (feature 14 > 26.5)\n",
      "       Predict: 214.04819277108433\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 3.5)\n",
      "       Predict: 741.64\n",
      "      Else (feature 14 > 3.5)\n",
      "       Predict: 309.03921568627453\n",
      "    Else (feature 13 > 4.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 2 in {1.0})\n",
      "       Predict: 300.0\n",
      "      Else (feature 2 not in {1.0})\n",
      "       Predict: 10000.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      If (feature 3 in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 222.91666666666666\n",
      "      Else (feature 3 not in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 398.0\n",
      "  Else (feature 12 > 2.5)\n",
      "   If (feature 1 in {0.0,1.0,2.0,3.0,4.0})\n",
      "    If (feature 12 <= 5.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 14 <= 7.5)\n",
      "       Predict: 493.3795620437956\n",
      "      Else (feature 14 > 7.5)\n",
      "       Predict: 296.76666666666665\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 9 <= -122.411075)\n",
      "       Predict: 722.96875\n",
      "      Else (feature 9 > -122.411075)\n",
      "       Predict: 2399.4\n",
      "    Else (feature 12 > 5.5)\n",
      "     If (feature 4 in {0.0,1.0,5.0,7.0})\n",
      "      If (feature 3 in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 609.5\n",
      "      Else (feature 3 not in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 1715.0\n",
      "     Else (feature 4 not in {0.0,1.0,5.0,7.0})\n",
      "      Predict: 8000.0\n",
      "   Else (feature 1 not in {0.0,1.0,2.0,3.0,4.0})\n",
      "    Predict: 8000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(labelCol=\"price\")\n",
    "# Filter for just numeric columns (and exclude price, our label)\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "# Combine output of StringIndexer defined above and numeric columns\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "# Combine stages into pipeline\n",
    "stages = [stringIndexer, vecAssembler, dt]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "dt.setMaxBins(40)\n",
    "pipelineModel = pipeline.fit(trainDF) # This line should error\n",
    "dtModel = pipelineModel.stages[-1]\n",
    "print(dtModel.toDebugString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "412a527e-36d0-4adb-aaac-a117d27ef97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.283406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policyIndex</td>\n",
       "      <td>0.167893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant_bookableIndex</td>\n",
       "      <td>0.140081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>property_typeIndex</td>\n",
       "      <td>0.128179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0.126233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansedIndex</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.038810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minimum_nights</td>\n",
       "      <td>0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.015218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>room_typeIndex</td>\n",
       "      <td>0.010905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accommodates</td>\n",
       "      <td>0.003603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_is_superhostIndex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bathrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beds_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>review_scores_rating_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>review_scores_accuracy_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>review_scores_cleanliness_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>review_scores_checkin_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>review_scores_communication_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>review_scores_location_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bedrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>host_total_listings_count</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bed_typeIndex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>review_scores_value_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           feature  importance\n",
       "12                        bedrooms    0.283406\n",
       "1         cancellation_policyIndex    0.167893\n",
       "2            instant_bookableIndex    0.140081\n",
       "4               property_typeIndex    0.128179\n",
       "15               number_of_reviews    0.126233\n",
       "3      neighbourhood_cleansedIndex    0.056200\n",
       "9                        longitude    0.038810\n",
       "14                  minimum_nights    0.029473\n",
       "13                            beds    0.015218\n",
       "5                   room_typeIndex    0.010905\n",
       "10                    accommodates    0.003603\n",
       "0           host_is_superhostIndex    0.000000\n",
       "24                    bathrooms_na    0.000000\n",
       "25                         beds_na    0.000000\n",
       "26         review_scores_rating_na    0.000000\n",
       "27       review_scores_accuracy_na    0.000000\n",
       "28    review_scores_cleanliness_na    0.000000\n",
       "22             review_scores_value    0.000000\n",
       "29        review_scores_checkin_na    0.000000\n",
       "30  review_scores_communication_na    0.000000\n",
       "31       review_scores_location_na    0.000000\n",
       "23                     bedrooms_na    0.000000\n",
       "16            review_scores_rating    0.000000\n",
       "21          review_scores_location    0.000000\n",
       "20     review_scores_communication    0.000000\n",
       "19           review_scores_checkin    0.000000\n",
       "18       review_scores_cleanliness    0.000000\n",
       "17          review_scores_accuracy    0.000000\n",
       "11                       bathrooms    0.000000\n",
       "8                         latitude    0.000000\n",
       "7        host_total_listings_count    0.000000\n",
       "6                    bed_typeIndex    0.000000\n",
       "32          review_scores_value_na    0.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understanding feature importance scores from our model to see the most important features\n",
    "import pandas as pd\n",
    "featureImp = pd.DataFrame(list(zip(vecAssembler.getInputCols(), \n",
    "                                   dtModel.featureImportances)),columns=[\"feature\", \"importance\"])\n",
    "featureImp.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f38b77-1573-424e-a6ba-8c24518cf87a",
   "metadata": {},
   "source": [
    "## Ways to improve model performance\n",
    "**Bootstrapping samples by rows**: Bootstrapping is a technique for simulating new data by sampling with replacement from your original data. Each decision tree is trained on a different bootstrap sample of your data set, which produces slightly different decision trees (M&M approach), and then you aggregate their predictions. This process is called ```bootstrap aggregating``` or ```bagging```.\n",
    "\n",
    "**Random feature selection by columns**: The main drawback with bagging is that the trees are all highly correlated, and thus learn similar patterns in your data. To mitigate this problem, each time you want to make a split you only consider a random subset of the columns (1/3 of the features for ```RandomForestRegressor``` and ```SquareRoot of features``` for ```RandomForestClassifier```). Due to this randomness you introduce, you typically want each tree to be quite shallow. You might be thinking: each of these trees will perform worse than any single decision tree, so how could this approach possibly be better? It turns out that each of the trees learns something different about your data set, and combining this collection of “weak” learners into an ensemble makes the forest much more robust than a single decision tree. \n",
    "\n",
    "So how do we determine what the optimal number of trees in our random forest or the max depth of those trees should be? This process is called ```hyperparameter tuning```.\n",
    "\n",
    "## Ways to tune hyperparameters\n",
    "**k-Fold Cross-Validation**: For example, instead of splitting our data into an 80/20 train/test split, as we did earlier, we can do a 60/20/20 split to generate training, validation, and test data sets, respectively. With this approach we lose 25% of the training data (80% to 60%) which could have been used to help improve the model. \n",
    "\n",
    "Instead the spiltting the data set into training, validation, and test sets, we spilt into training and test sets as before -but we use the training data for both training and validation. To accomplish this, we split our training data into ```k``` subsets, or “folds” (e.g., three). Then, for a given hyperparameter configuration,\n",
    "we train our model on ```k–1``` folds and evaluate on the remaining fold, repeating this process k times. \n",
    "\n",
    "|  |```k-2``` Folds| ```k-1``` Folds |```k``` Folds|\n",
    "|---|--------|-------|-------|\n",
    "|Pass 1:|Train|Train|Validate|\n",
    "|Pass 2:|Train|Validate|Train|\n",
    "|Pass 3:|Validate|Train|Train|\n",
    "\n",
    "As this figure shows, if we split our data into three folds, our model is first trained on the first and second folds (or splits) of the data, and evaluated on the third fold. We then build the same model with the same hyperparameters on the first and third folds of the data, and evaluate its performance on the second fold. Lastly, we build the model on the second and third folds and evaluate it on the first fold. We then average the performance of those three (or k) validation data sets as a proxy of how well this model will perform on unseen data, as every data point had the chance to be part of the validation data set exactly once. Next, we repeat this process for all of our different hyperparameter configurations to identify the optimal one.\n",
    "\n",
    "To perform a hyperparameter search in Spark, take the following steps :\n",
    "- Define the estimator you want to evaluate.\n",
    "- Specify which hyperparameters you want to vary, as well as their respective values, using the ```ParamGridBuilder```.\n",
    "- Define an evaluator to specify which metric to use to compare the various models.\n",
    "- Use the ```CrossValidator``` to perform cross-validation, evaluating each of the various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579e1fe7-d89b-4f9f-bcf6-cac5629b3b69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14392/2143233590.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Define a pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStringIndexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvecAssembler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuning\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParamGridBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# vary our maxDepth to be 2, 4, or 6 and numTrees to be 10 and 100.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a pipeline \n",
    "pipeline = Pipeline(stages = [StringIndexer, vecAssembler, rf])\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "# vary our maxDepth to be 2, 4, or 6 and numTrees to be 10 and 100.\n",
    "# retun the 3X2 matrix with different hyperparameter configurations \n",
    "paramGrid = (ParamGridBuilder().addgrid(rf.maxDepth, [2, 5, 6])\n",
    "                             .addGrid(rf.numTrees, [10,100])\n",
    "                             .build())\n",
    "\n",
    "# Now that we have set up our hyperparameter grid, we need to define how to evaluate \n",
    "# each of the models to determine which one performed best. For this task we will use\n",
    "# the RegressionEvaluator, and we’ll use RMSE as our metric of interest:\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# perfroming k-fold cross-validation and fit it with cross-validator to the training data set \n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator,estimatorParamMaps=paramGrid, numFolds=3, seed=42)\n",
    "cvModel = cv.fit(trainDF)\n",
    "\n",
    "# we just train total 19 models:  6 hyperparameters (3X2 metirc) with 3 folds. Plus, Spark retrains\n",
    "# your model on the entire training data set once it has identified the optimal \n",
    "# hyperparameter configuration, so in the end we trained 19 models. If you want to retain the \n",
    "# intermediate models trained, you can set collect SubModels=True in the CrossValidator\n",
    "\n",
    "# inspect the results of the cross-validator\n",
    "# Best model the one with lowest RMSE\n",
    "list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))\n",
    "\n",
    "# Optimizing Pipelines\n",
    "# to faster the trainin time, we can pass parallelism into the model. \n",
    "# Generally speaking, a value up to 10 should be sufficient for most clusters\n",
    "cvModel = cv.setParallelism(4).fit(trainDF)\n",
    "\n",
    "# Further spped up the training time: Nesting inside altogether\n",
    "cv = CrossValidator(estimator=rf, \n",
    "                    evaluator=evaluator, \n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=3,\n",
    "                    parallelism=4,\n",
    "                    seed=42)\n",
    "pipeline = Pipeline(stages=[stringIndexer, vecAssembler, cv])\n",
    "pipelineModel = pipeline.fit(trainDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb2f7a-2495-4042-8f63-582f5263feae",
   "metadata": {},
   "source": [
    "## Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754e506-8e35-4d41-b3a1-dc13cc8ea43c",
   "metadata": {},
   "source": [
    "## MLfow\n",
    "\n",
    "MLflow is an open source platform that helps developers reproduce and share experiments, \n",
    "manage models, and much more\n",
    "- Tracking: Provides APIs to record parameters, metrics, code versions, models, and artifacts such as plots, and text\n",
    "- Projects: A standardized format to package your data science projects and their dependencies to run on other platforms\n",
    "- Models: A standardized format to package models to deploy to diverse execution environments. It provides a consistent API for loading and applying models, regardless of the algorithm or library used to build the model.\n",
    "- Registry: A repository to keep track of model lineage, model versions, stage transitions, and annotations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
