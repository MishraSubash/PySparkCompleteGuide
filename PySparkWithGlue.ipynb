{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 12,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 22b17c98-80fd-46fc-8fd7-f6f44c01fa39\nApplying the following default arguments:\n--glue_kernel_version 1.0.2\n--enable-glue-datacatalog true\nWaiting for session 22b17c98-80fd-46fc-8fd7-f6f44c01fa39 to get into ready status...\nSession 22b17c98-80fd-46fc-8fd7-f6f44c01fa39 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# Read from the customers table in the glue data catalog using a dynamic frame\ndynamicFrameCustomers = glueContext.create_dynamic_frame.from_catalog(\ndatabase = \"pyspark_tutorial_db\", \ntable_name = \"customers\"\n)\n\n# Show the top 10 rows from the dynamic dataframe\ndynamicFrameCustomers.show(3)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "{\"customerid\": 293, \"firstname\": \"Catherine\", \"lastname\": \"Abel\", \"fullname\": \"Catherine Abel\"}\n{\"customerid\": 295, \"firstname\": \"Kim\", \"lastname\": \"Abercrombie\", \"fullname\": \"Kim Abercrombie\"}\n{\"customerid\": 297, \"firstname\": \"Humberto\", \"lastname\": \"Acevedo\", \"fullname\": \"Humberto Acevedo\"}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df_customer = dynamicFrameCustomers.toDF()\ndf_customer.show(3)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+---------+-----------+----------------+\n|customerid|firstname|   lastname|        fullname|\n+----------+---------+-----------+----------------+\n|       293|Catherine|       Abel|  Catherine Abel|\n|       295|      Kim|Abercrombie| Kim Abercrombie|\n|       297| Humberto|    Acevedo|Humberto Acevedo|\n+----------+---------+-----------+----------------+\nonly showing top 3 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Some DF processing\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# Selecting certain fields from a Dynamic DataFrame\ndf_select = df_customer.select([\"customerid\", \"fullname\"])\n\n#Drop Fields of Dynamic Frame\n#df_customer = df_customer.drop([\"firstname\",\"lastname\"])\n\n# Show top 10\ndf_select.show(2)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+---------------+\n|customerid|       fullname|\n+----------+---------------+\n|       293| Catherine Abel|\n|       295|Kim Abercrombie|\n+----------+---------------+\nonly showing top 2 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Add Columns In A Spark Dataframe",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "#import lit from sql functions \nfrom  pyspark.sql.functions import lit\n\n# Add new column to spark dataframe\ndfNewColumn = df_customer.withColumn(\"date\", lit(\"2022-07-24\"))\n\n# show df with new column\ndfNewColumn.show(3)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+---------+-----------+----------------+----------+\n|customerid|firstname|   lastname|        fullname|      date|\n+----------+---------+-----------+----------------+----------+\n|       293|Catherine|       Abel|  Catherine Abel|2022-07-24|\n|       295|      Kim|Abercrombie| Kim Abercrombie|2022-07-24|\n|       297| Humberto|    Acevedo|Humberto Acevedo|2022-07-24|\n+----------+---------+-----------+----------------+----------+\nonly showing top 3 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Using concat to concatenate two columns together",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "#import concat from functions \nfrom  pyspark.sql.functions import concat\n\n# create another full name column\ndfNewFullName = df_customer.withColumn(\"new_full_name\",concat(\"firstname\",concat(lit(' '),\"lastname\")))\n\n#show full name column \ndfNewFullName.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+---------+-----------+----------------+----------------+\n|customerid|firstname|   lastname|        fullname|   new_full_name|\n+----------+---------+-----------+----------------+----------------+\n|       293|Catherine|       Abel|  Catherine Abel|  Catherine Abel|\n|       295|      Kim|Abercrombie| Kim Abercrombie| Kim Abercrombie|\n|       297| Humberto|    Acevedo|Humberto Acevedo|Humberto Acevedo|\n|       291|  Gustavo|     Achong|  Gustavo Achong|  Gustavo Achong|\n|       299|    Pilar|   Ackerman|  Pilar Ackerman|  Pilar Ackerman|\n+----------+---------+-----------+----------------+----------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Renaming columns",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Rename column in Spark dataframe\ndfRenameCol = df_customer.withColumnRenamed(\"fullname\",\"full_name\")\n\n#show renamed column dataframe\ndfRenameCol.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+---------+-----------+----------------+\n|customerid|firstname|   lastname|       full_name|\n+----------+---------+-----------+----------------+\n|       293|Catherine|       Abel|  Catherine Abel|\n|       295|      Kim|Abercrombie| Kim Abercrombie|\n|       297| Humberto|    Acevedo|Humberto Acevedo|\n|       291|  Gustavo|     Achong|  Gustavo Achong|\n|       299|    Pilar|   Ackerman|  Pilar Ackerman|\n+----------+---------+-----------+----------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: GroupBy and Aggregate Operations",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Group by lastname then print counts of lastname and show\ndf_customer.groupBy(\"lastname\").count().show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------+-----+\n|lastname|count|\n+--------+-----+\n|  Achong|    1|\n|  Bailey|    1|\n|   Caron|    1|\n|   Casts|    1|\n|   Curry|    1|\n+--------+-----+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Filtering Columns and Where clauses",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Filter spark DataFrame for customers who have the last name Adams\n# Using Filter method\ndf_customer.filter(df_customer[\"lastname\"] == \"Adams\").show()\n\n# using where clause\n# Where clause spark DataFrame for customers who have the last name Adams\ndf_customer.where(\"lastname =='Adams'\").show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+---------+--------+-------------+\n|customerid|firstname|lastname|     fullname|\n+----------+---------+--------+-------------+\n|       305|    Carla|   Adams|  Carla Adams|\n|       301|  Frances|   Adams|Frances Adams|\n|       307|      Jay|   Adams|    Jay Adams|\n+----------+---------+--------+-------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Joins",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Read up another datset ordersand convert to spark dataframe\n# Read from the customers table in the glue data catalog using a dynamic frame and convert to spark dataframe\ndfOrders = glueContext.create_dynamic_frame.from_catalog(\n                                        database = \"pyspark_tutorial_db\", \n                                        table_name = \"orders\"\n                                    ).toDF()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#### Example: Inner join for Spark Dataframe All Data\n# Inner Join Customers Spark DF to Orders Spark DF\ndf_combine = df_customer.join(dfOrders,df_customer.customerid ==  dfOrders.customerid,\"inner\")\n#left join on orders and adams df\n# df_customer.join(dfOrders,df_customer.customerid ==  dfOrders.customerid,\"left\").show(5, truncate=False)\ndf_combine.show(5, truncate= False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+---------+--------+--------------+------------+------------------+---------+---------+--------+----------+----------+----------+---------+--------+----------+---------+--------+---------+-----------------+---------+\n|customerid|firstname|lastname|fullname      |salesorderid|salesorderdetailid|orderdate|duedate  |shipdate|employeeid|customerid|subtotal  |taxamt   |freight |totaldue  |productid|orderqty|unitprice|unitpricediscount|linetotal|\n+----------+---------+--------+--------------+------------+------------------+---------+---------+--------+----------+----------+----------+---------+--------+----------+---------+--------+---------+-----------------+---------+\n|517       |Richard  |Bready  |Richard Bready|43665       |61                |5/31/2011|6/12/2011|6/7/2011|283       |517       |14352.7713|1375.9427|429.9821|16158.6961|711      |2       |20.1865  |0.0000           |40.3730  |\n|517       |Richard  |Bready  |Richard Bready|43665       |62                |5/31/2011|6/12/2011|6/7/2011|283       |517       |14352.7713|1375.9427|429.9821|16158.6961|773      |1       |2039.9940|0.0000           |2039.9940|\n|517       |Richard  |Bready  |Richard Bready|43665       |63                |5/31/2011|6/12/2011|6/7/2011|283       |517       |14352.7713|1375.9427|429.9821|16158.6961|707      |1       |20.1865  |0.0000           |20.1865  |\n|517       |Richard  |Bready  |Richard Bready|43665       |64                |5/31/2011|6/12/2011|6/7/2011|283       |517       |14352.7713|1375.9427|429.9821|16158.6961|715      |2       |28.8404  |0.0000           |57.6808  |\n|517       |Richard  |Bready  |Richard Bready|43665       |65                |5/31/2011|6/12/2011|6/7/2011|283       |517       |14352.7713|1375.9427|429.9821|16158.6961|777      |2       |2024.9940|0.0000           |4049.9880|\n+----------+---------+--------+--------------+------------+------------------+---------+---------+--------+----------+----------+----------+---------+--------+----------+---------+--------+---------+-----------------+---------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Writing data down using the Glue Data Catalog",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "##### Note: convert from spark Dataframe to Glue Dynamic DataFrame",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "# Import Dynamic DataFrame class\nfrom awsglue.dynamicframe import DynamicFrame\n\n#Convert from Spark Data Frame to Glue Dynamic Frame\ndyfcombined = DynamicFrame.fromDF(df_combine, glueContext, \"convert\")\n\n#Show converted Glue Dynamic Frame\ndyfcombined.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "{\"customerid\": 295, \"firstname\": \"Kim\", \"lastname\": \"Abercrombie\", \"fullname\": \"Kim Abercrombie\", \"salesorderid\": 44110, \"salesorderdetailid\": 1732, \"orderdate\": \"8/1/2011\", \"duedate\": \"8/13/2011\", \"shipdate\": \"8/8/2011\", \"employeeid\": 277, \"subtotal\": 16667.3077, \"taxamt\": 1600.6864, \"freight\": 500.2145, \"totaldue\": 18768.2086, \"productid\": 765, \"orderqty\": 2, \"unitprice\": 419.4589, \"unitpricediscount\": 0.0000, \"linetotal\": 838.9178}\n{\"customerid\": 295, \"firstname\": \"Kim\", \"lastname\": \"Abercrombie\", \"fullname\": \"Kim Abercrombie\", \"salesorderid\": 44110, \"salesorderdetailid\": 1733, \"orderdate\": \"8/1/2011\", \"duedate\": \"8/13/2011\", \"shipdate\": \"8/8/2011\", \"employeeid\": 277, \"subtotal\": 16667.3077, \"taxamt\": 1600.6864, \"freight\": 500.2145, \"totaldue\": 18768.2086, \"productid\": 729, \"orderqty\": 4, \"unitprice\": 183.9382, \"unitpricediscount\": 0.0000, \"linetotal\": 735.7528}\n{\"customerid\": 295, \"firstname\": \"Kim\", \"lastname\": \"Abercrombie\", \"fullname\": \"Kim Abercrombie\", \"salesorderid\": 44110, \"salesorderdetailid\": 1734, \"orderdate\": \"8/1/2011\", \"duedate\": \"8/13/2011\", \"shipdate\": \"8/8/2011\", \"employeeid\": 277, \"subtotal\": 16667.3077, \"taxamt\": 1600.6864, \"freight\": 500.2145, \"totaldue\": 18768.2086, \"productid\": 714, \"orderqty\": 1, \"unitprice\": 28.8404, \"unitpricediscount\": 0.0000, \"linetotal\": 28.8404}\n{\"customerid\": 295, \"firstname\": \"Kim\", \"lastname\": \"Abercrombie\", \"fullname\": \"Kim Abercrombie\", \"salesorderid\": 44110, \"salesorderdetailid\": 1735, \"orderdate\": \"8/1/2011\", \"duedate\": \"8/13/2011\", \"shipdate\": \"8/8/2011\", \"employeeid\": 277, \"subtotal\": 16667.3077, \"taxamt\": 1600.6864, \"freight\": 500.2145, \"totaldue\": 18768.2086, \"productid\": 715, \"orderqty\": 5, \"unitprice\": 28.8404, \"unitpricediscount\": 0.0000, \"linetotal\": 144.2020}\n{\"customerid\": 295, \"firstname\": \"Kim\", \"lastname\": \"Abercrombie\", \"fullname\": \"Kim Abercrombie\", \"salesorderid\": 44110, \"salesorderdetailid\": 1736, \"orderdate\": \"8/1/2011\", \"duedate\": \"8/13/2011\", \"shipdate\": \"8/8/2011\", \"employeeid\": 277, \"subtotal\": 16667.3077, \"taxamt\": 1600.6864, \"freight\": 500.2145, \"totaldue\": 18768.2086, \"productid\": 760, \"orderqty\": 3, \"unitprice\": 419.4589, \"unitpricediscount\": 0.0000, \"linetotal\": 1258.3767}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Example: Write output data to S3 location",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: 1. Write Dynamic DataFrame down to S3 location",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# write down the data in converted Dynamic Frame to S3 location. \nglueContext.write_dynamic_frame.from_options(\n                            frame = dyfcombined,\n                            connection_type=\"s3\", \n                            connection_options = {\"path\": \"s3://pyspark-with-glue-learning-20231202\"}, \n                            format = \"csv\", \n                            format_options={\n                                \"separator\": \",\"\n                                },\n    # transformation_ctx which is used to identify state information for a job bookmark.\n                            transformation_ctx = \"datasink2\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f5d621420e0>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: 2. Write Dynamic DataFrame down to S3 location",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# write data from spark dataframe to the table using the meta data stored in the glue data catalog \nglueContext.write_dynamic_frame.from_catalog(\n    frame = dyfcombined,\n    database = \"pyspark_tutorial_db\",  \n    table_name = \"customers_write_dyf\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f5d62141ea0>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://bucket_name/folder_name\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"demo\", catalogTableName=\"populations\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(DyF)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		}
	]
}